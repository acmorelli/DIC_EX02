{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e85a29-3f82-4349-a3d8-a0b9aaa74f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector, StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67632f60-e1b7-414b-a607-cd4fb7ceb811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(asin='0981850006', category='Patio_Lawn_and_Garde', helpful=[6, 7], overall=5.0, reviewText=\"This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!\", reviewTime='12 3, 2009', reviewerID='A2VNYWOPJ13AFP', reviewerName='Amazon Customer \"carringt0n\"', summary='Delish', unixReviewTime=1259798400)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"task2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fa87f62-5788-4e1c-b2c9-02c2ce4cd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"reviewText\",\n",
    "    outputCol=\"tokens\",\n",
    "    pattern=r\"[\\s\\d()\\[\\]{}\\.\\!\\?,;:+=\\-_\\\"'`~#@&\\*\\%€\\$§\\\\/]+\"\n",
    ")\n",
    "\n",
    "\n",
    "stopwords = spark.sparkContext.textFile(\n",
    "    \"hdfs:///user/e01652446/input/stopwords.txt\"\n",
    ").collect()\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\", stopWords=stopwords)\n",
    "\n",
    "\n",
    "CountVec = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidf\")\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "\n",
    "selector = ChiSqSelector(\n",
    "    numTopFeatures=2000,\n",
    "    featuresCol=\"tfidf\",\n",
    "    outputCol=\"selectedFeatures\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fba6e13-c22d-4b17-b0b6-58614fdf806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 22:16:14 WARN DAGScheduler: Broadcasting large task binary with size 1063.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 22:16:28 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/05/12 22:16:29 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 22:16:41 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    CountVec,\n",
    "    idf,\n",
    "    indexer,    \n",
    "    selector   \n",
    "])\n",
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21926d5c-3377-43aa-bbb4-8829762059bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.stages[2].vocabulary \n",
    "selected_indices = model.stages[-1].selectedFeatures\n",
    "\n",
    "selected_terms = [vocab[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e002fac6-a9bf-429b-9a32-748ceac64c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.parallelize(selected_terms, 1) \\\n",
    "  .saveAsTextFile(\"output_ds.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0eae36-8f21-47f6-8495-64b4c09b2aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
